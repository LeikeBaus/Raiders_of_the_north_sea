ğŸ§± Projektarchitektur (High Level)
/Raiders_of_the_north_sea
â”‚
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ state.py          # kompletter Spielzustand
â”‚   â”œâ”€â”€ rules.py          # Regel-Engine
â”‚   â”œâ”€â”€ actions.py        # formale Aktionsdefinitionen
â”‚   â”œâ”€â”€ cards.py          # Townsfolk, Crew, Offerings
â”‚   â”œâ”€â”€ board.py          # Village + Raids
â”‚   â””â”€â”€ engine.py         # Turn-Loop, Endbedingungen
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ random_agent.py
â”‚   â”œâ”€â”€ heuristic_agent.py
â”‚   â”œâ”€â”€ rl_agent.py
â”‚
â”œâ”€â”€ rl_env/
â”‚   â””â”€â”€ raiders_env.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ selfplay.py
â”‚   â”œâ”€â”€ train.py
â”‚
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ stats.py
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ gui.py
â”‚
â””â”€â”€ README.md

âš™ï¸ Phase 1 â€“ Spiel-Engine (deterministisch + simulativ)
Ziel

Eine vollstÃ¤ndig regelkonforme Simulation ohne UI.

Kernkonzepte
Spielzustand (GameState)

EnthÃ¤lt u.a.:

Spieler:

Ressourcen (Silber, Provisions, Plunder)

Handkarten

Crew

Armour

Valkyrie-Track

VP

Worker in Hand

Board:

Village-GebÃ¤ude + belegte Worker

Raiding-Spaces inkl. Plunder

Kartenstapel

Offering Tiles

RundenzÃ¤hler + Endbedingungen

Aktionen

Formalisieren als diskrete Aktionen:

Work-Phase

PlaceWorker(building)

PickupWorker(building)

PlayCard(card)

HireCrew(card)

BuyArmour(...)

TakeResources(...)

Raid-Phase

Raid(settlement, worker_color)

Alle Aktionen mÃ¼ssen:

âœ” legalitÃ¤tsgeprÃ¼ft werden
âœ” den GameState transformieren

Wichtige Designentscheidung

ğŸ‘‰ Trenne strikt:

Regellogik (pure functions)

State (immutable oder kontrolliert mutierend)

Das ist extrem wichtig fÃ¼r RL + Replays + Debugging.

ğŸ¤– Phase 2 â€“ Reinforcement Learning Environment
Ziel

Kompatibel zu Gymnasium/OpenAI Gym Stil.

obs, reward, done, info = env.step(action)

Beobachtungsraum (Observation Space)

Empfohlen: Vektorisiert + normalisiert.

Beispiel:

Eigene Ressourcen

Gegner-Ressourcen (aggregiert oder einzeln)

Crew-Komposition

verfÃ¼gbare Worker-PlÃ¤tze

verbleibende Plunder

Offering Tiles

Spielphase

Optional:
â¡ï¸ separate Feature-Gruppen (Board, Player, Global)

Aktionsraum

Diskret, z.B.:

0-20   Work-Aktionen
21-40  Raid-Aktionen
...


Oder hierarchisch (Advanced):

Erst Phase wÃ¤hlen

dann konkrete Aktion

Reward-Design

Minimalstart:

âœ… + Endgame VP Differenz

SpÃ¤ter shaping:

VP wÃ¤hrend Spiel

effiziente Raids

Offering completion

Vermeide zu starkes shaping â†’ sonst lernt KI falsche Strategien.

ğŸ” Phase 3 â€“ Self Play Training
Ablauf

KI spielt tausende Spiele gegen sich selbst

Policy wird nach jedem Batch verbessert

Optional:

Elo-Ranking der Agenten

Population Based Training

Algorithmen (empfohlen)

Start einfach:

PPO (stable-baselines3)

spÃ¤ter evtl.:

AlphaZero-Style (MCTS + NN)

ğŸ“Š Phase 4 â€“ Statistik & Analyse
Erfasste Daten pro Spiel
Nach Spieleranzahl getrennt:
Aktionen

HÃ¤ufigkeit:

Worker platzieren (GebÃ¤ude)

Worker aufnehmen

Raid-Typen

Karten

Gespielte Townsfolk

Gehirete Crew

Kombinationen

Aktion A â†’ Aktion B

Karte + folgende Aktion

Crew-Kombinationen

Erfolgsmessung

Winrate pro Aktion

Winrate pro Karte

Winrate pro Kombi

Beispiel:

Raid Monastery mit >=20 StÃ¤rke â†’ 63% Winrate
Sage + Offering Rush â†’ 71% Winrate

Tools

pandas

matplotlib / seaborn

optional: Jupyter notebooks

ğŸ® Phase 5 â€“ Mensch gegen KI Modus
Funktionen

âœ… Anzeige:

aktueller Spielstand

Ressourcen

Crew

âœ… KI:

berechnet:

Sieg-Wahrscheinlichkeit (Value Network)

beste Aktion (Policy)

Beispiel:

Win chance:
You: 42%
AI: 58%

Recommended move:
Place worker at Silversmith â†’ Pickup Town Hall
(Expected value +1.7 VP)

ğŸ–¥ UI (bewusst einfach halten)

Optionen:

pygame

Wichtig:
â¡ï¸ UI darf niemals Kernlogik enthalten

Nur State visualisieren + Inputs weiterreichen.

ğŸ“ˆ Erweiterungen (optional spÃ¤ter)

Erweiterungen vom Brettspiel

Menschliche Heuristik-Agenten

Explainable AI (warum Zug empfohlen?)

Replay Viewer

ğŸ§ª Testing

Unbedingt:

Unit Tests fÃ¼r Regeln

deterministische Seeds

Replaybarkeit

Beispiele:

Ressourcen nie negativ

illegale ZÃ¼ge blockiert

Endbedingungen korrekt

ğŸ—“ Grober Zeitplan (realistisch)
Phase	Aufwand
Spiel-Engine	2â€“4 Wochen
RL-Env	1 Woche
Training	1â€“3 Wochen
Statistik	1 Woche
UI	2â€“4 Tage
ğŸ“Œ Zentrale Erfolgsfaktoren

âœ… Saubere ZustandsreprÃ¤sentation
âœ… Trennung von Logik & Darstellung
âœ… Einfach starten, dann iterativ verfeinern
âœ… Erst random + heuristics â†’ dann RL